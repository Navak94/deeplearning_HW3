{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "#sanity test to ensure my desktop is picking my rtx 3080\n",
        "\n",
        "#comment it out if it's acting weird for you Tom\n",
        "###############################################################################\n",
        "\n",
        "#print(\"tom im checking if using GPU or CPU\")\n",
        "\n",
        "#print(\"sees CUDA device:\", torch.cuda.is_available())\n",
        "#print(\"CUDA Version:\", torch.version.cuda)\n",
        "#if torch.cuda.is_available():\n",
        "#    print(\"GPU in Use:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#end GPU check"
      ],
      "metadata": {
        "id": "YmuD4LZ5T4ZR"
      },
      "id": "YmuD4LZ5T4ZR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(input_dim, n_layers, hidden_units, output_dim):\n",
        "    \"\"\"Build an MLP with `n_layers` hidden linear+ReLU layers and a linear output.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        The constructed MLP.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    # BEGIN YOUR CODE HERE (~5-6 lines)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        layers += [nn.Linear(input_dim,hidden_units), nn.ReLU()]\n",
        "        input_dim = hidden_units\n",
        "\n",
        "    #Y_HAT = nn.Softmax(layers[-1]) I think I need to do this somewhere?  like softmax was in the ssignment\n",
        "    layers += [nn.Linear(input_dim, output_dim)]\n",
        "\n",
        "\n",
        "    # END YOUR CODE HERE\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def extract_model_params(model):\n",
        "    \"\"\"Extract all parameters of a PyTorch model.\"\"\"\n",
        "    return torch.cat([p.view(-1) for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def load_params_into_model(model, all_params):\n",
        "    \"\"\"Loads a flattened array of parameters back into a PyTorch model.\"\"\"\n",
        "    current_pos = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            # Reshape the flattened parameters to the original shape of the parameter\n",
        "            param.data.copy_(all_params[current_pos : current_pos + num_params].view(param.size()))\n",
        "            current_pos += num_params\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X, y in loader:\n",
        "        # BEGIN YOUR CODE HERE (~5-7 lines)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # END YOUR CODE HERE\n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "    # Extract and store the model parameters after the epoch\n",
        "    all_params = extract_model_params(model)\n",
        "\n",
        "    return running_loss / len(loader.dataset), all_params.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            # BEGIN YOUR CODE HERE (~4 lines)\n",
        "\n",
        "            logits = model(X)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "\n",
        "\n",
        "            # END YOUR CODE HERE\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def compute_loss(model, loader, criterion):\n",
        "    running_loss = 0.0\n",
        "    for X, y in loader:\n",
        "        # BEGIN YOUR CODE HERE (~3 lines)\n",
        "        with torch.no_grad():\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits, y)\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        # END YOUR CODE HERE\n",
        "    return running_loss / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "vrykmoCcT6hW"
      },
      "id": "vrykmoCcT6hW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data (numpy arrays assumed present in workspace)\n",
        "X_train = np.load(\"fashion_mnist_train_images.npy\").astype(np.float32) / 255.0\n",
        "y_train = np.load(\"fashion_mnist_train_labels.npy\").astype(np.int64)\n",
        "X_test = np.load(\"fashion_mnist_test_images.npy\").astype(np.float32) / 255.0\n",
        "y_test = np.load(\"fashion_mnist_test_labels.npy\").astype(np.int64)\n",
        "\n",
        "# Flatten if images are HxW\n",
        "if X_train.ndim == 3:\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# center data similarly to previous versions\n",
        "X_train = X_train - 0.5\n",
        "X_test = X_test - 0.5\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.from_numpy(X_train)\n",
        "y_train_t = torch.from_numpy(y_train)\n",
        "X_test_t = torch.from_numpy(X_test)\n",
        "y_test_t = torch.from_numpy(y_test)\n",
        "\n",
        "# SEED FIX\n",
        "torch.manual_seed(541)\n",
        "np.random.seed(541)\n",
        "\n",
        "# Train/val split\n",
        "full_train = TensorDataset(X_train_t, y_train_t)\n",
        "val_size = int(0.2 * len(full_train))\n",
        "train_size = len(full_train) - val_size\n",
        "train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n"
      ],
      "metadata": {
        "id": "qfDW5FaTUIlJ"
      },
      "id": "qfDW5FaTUIlJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparam_tuning(train_dataset, val_dataset, seed=541):\n",
        "    \"\"\"Systematically search hyperparameters and return the best config and model state.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict, state_dict\n",
        "        Best hyperparameters and the corresponding model state dict.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    best_cfg = None\n",
        "    best_acc = 0.0\n",
        "    # BEGIN YOUR CODE HERE (~15-20 lines)\n",
        "\n",
        "    #Tom feel free to adjust these if that particular run chokes on your laptop's hardware\n",
        "    #batch size, hidden, and layers are more likely to make stuff slug in my experience\n",
        "\n",
        "    batch_sizes = [8,12,16,20,32,48,64,128,145,256]\n",
        "    learning_rates= [0.01,0.005,0.001,0.0005,0.0001,0.00005,0.00001]\n",
        "    layers = [2,3,4,5,6]\n",
        "    hidden = [5,10,20,30,40]\n",
        "    alphas = [0,0.0005,0.001,0.0001,0.00001]\n",
        "    epochs = [15, 20, 25]\n",
        "\n",
        "\n",
        "    #########################################\n",
        "    x0, y0 = train_dataset[0]\n",
        "    n_features = x0.numel()\n",
        "    n_classes  = 10\n",
        "    ##############################################3\n",
        "\n",
        "\n",
        "    loopcount = 0\n",
        "    #try 10 random configurations adjust parameters and get the highest accuracy\n",
        "    config_attempts = 10\n",
        "    #you can change this as needed my dude\n",
        "\n",
        "\n",
        "    for _ in range(config_attempts):\n",
        "\n",
        "\n",
        "        layer_choice =  int(np.random.choice(layers).item())\n",
        "        batch_size_choice = int(np.random.choice(batch_sizes).item())\n",
        "        learning_rate_choice =float(np.random.choice(learning_rates).item())\n",
        "        hidden_choice = int(np.random.choice(hidden).item())\n",
        "        aplha_choice = float(np.random.choice(alphas).item())\n",
        "        epoch_choices = int(np.random.choice(epochs).item())\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size_choice, shuffle=True)\n",
        "        val_loader   = DataLoader(val_dataset,   batch_size=batch_size_choice, shuffle=False)\n",
        "\n",
        "        # SEED FIX: unique seed for each model initialization for reproducibility\n",
        "        torch.manual_seed(seed + loopcount)\n",
        "        model = build_mlp(n_features, layer_choice,hidden_choice, n_classes)\n",
        "        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_choice, weight_decay=aplha_choice)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        ############################################################################# needs revision\n",
        "\n",
        "\n",
        "        weight_params =[]\n",
        "        bias_params = []\n",
        "        for name, p in model.named_parameters():\n",
        "            (weight_params if 'weight' in name else bias_params).append(p)\n",
        "        optimizer = torch.optim.SGD([{'params': weight_params, 'weight_decay': aplha_choice},{'params': bias_params,   'weight_decay': 0.0}],lr=learning_rate_choice)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################################################################################\n",
        "\n",
        "        for _e in range(epoch_choices):\n",
        "            train_epoch(model, train_loader, criterion, optimizer)\n",
        "\n",
        "        current_accuracy = evaluate(model, val_loader)\n",
        "\n",
        "        #kind of a stupid sanity check to see what loop im on and to ensure its not like frozen\n",
        "        loopcount = loopcount +1\n",
        "        #print(\"on loop \", loopcount)# turned off for final output\n",
        "\n",
        "        if current_accuracy > best_acc:\n",
        "            best_acc = current_accuracy\n",
        "            print(\"best accuracy \",best_acc)\n",
        "            #best_configs = [layer_choice , batch_size_choice , learning_rate_choice, hidden_choice , aplha_choice , epoch_choices]\n",
        "\n",
        "            best_cfg = {\n",
        "                    \"layers\": layer_choice,\n",
        "                    \"hidden\": hidden_choice,\n",
        "                    \"batch\": batch_size_choice,\n",
        "                    \"learning_rate\": learning_rate_choice,\n",
        "                    \"alpha\": aplha_choice,\n",
        "                    \"epochs\": epoch_choices,\n",
        "                    \"seed\": seed + loopcount - 1  # SEED FIX\n",
        "                }\n",
        "\n",
        "            print(\"New best:\", best_cfg, \"val_acc=\", f\"{best_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    return best_cfg, best_acc\n",
        "\n",
        "\n",
        "# Run hyperparameter tuning\n",
        "best_cfg, best_acc = hyperparam_tuning(train_dataset, val_dataset)\n",
        "print(f\"Best config: {best_cfg} with val_acc={best_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPPtO-pgUJMX",
        "outputId": "b5762612-1f0a-4599-aaab-23e66fdb0f78",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759351148690,
          "user_tz": 240,
          "elapsed": 311,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "GPPtO-pgUJMX",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best accuracy  0.2663333333333333\n",
            "New best: {'layers': 2, 'hidden': 5, 'batch': 20, 'learning_rate': 0.0001, 'alpha': 1e-05, 'epochs': 20, 'seed': 541} val_acc= 0.2663\n",
            "best accuracy  0.2911666666666667\n",
            "New best: {'layers': 6, 'hidden': 5, 'batch': 20, 'learning_rate': 0.001, 'alpha': 0.0005, 'epochs': 25, 'seed': 544} val_acc= 0.2912\n",
            "best accuracy  0.85925\n",
            "New best: {'layers': 4, 'hidden': 40, 'batch': 8, 'learning_rate': 0.001, 'alpha': 0.0001, 'epochs': 25, 'seed': 548} val_acc= 0.8592\n",
            "Best config: {'layers': 4, 'hidden': 40, 'batch': 8, 'learning_rate': 0.001, 'alpha': 0.0001, 'epochs': 25, 'seed': 548} with val_acc=0.8592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain best model on full training set (train+val) for more epochs\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = int(y_train.max() + 1)\n",
        "# Instantiate the best model\n",
        "# BEGIN YOUR CODE HERE (~3 lines)\n",
        "\n",
        "\n",
        "torch.manual_seed(best_cfg['seed']) # SEED FIX\n",
        "best_model = build_mlp(\n",
        "    n_features,\n",
        "    best_cfg['layers'],\n",
        "    best_cfg['hidden'],\n",
        "    n_classes\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "# create full training loader and test loader\n",
        "batch_size = best_cfg['batch']\n",
        "full_train_loader = DataLoader(full_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(best_model.parameters(),\n",
        "                          lr=best_cfg['learning_rate'],\n",
        "                          weight_decay=best_cfg['alpha'])\n",
        "# retrain best model\n",
        "epochs_final = 50\n",
        "parameter_history = []  # To store the history of parameters\n",
        "for epoch in range(epochs_final):\n",
        "    # BEGIN YOUR CODE HERE (~2 lines)\n",
        "\n",
        "    #needs revision\n",
        "    t0 = __import__('time').time(); loss, flat_params = train_epoch(best_model, full_train_loader, criterion, optimizer)\n",
        "    parameter_history.append(flat_params); test_acc = evaluate(best_model, test_loader); t1 = __import__('time').time()\n",
        "\n",
        "\n",
        "    # END YOUR CODE HERE\n",
        "    print(f\"Final Train Epoch {epoch+1}/{epochs_final}: loss={loss:.4f}, test_acc={test_acc:.4f}, time={t1-t0:.1f}s\")\n",
        "\n",
        "# Print the length of the parameter_history list\n",
        "print(f\"Length of parameter history: {len(parameter_history)}\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jD8-lKkiUNPz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759351148690,
          "user_tz": 240,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4fc2016b-c029-49c2-935a-e23ccced9a9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jD8-lKkiUNPz",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Train Epoch 1/50: loss=2.2452, test_acc=0.2274, time=9.7s\n",
            "Final Train Epoch 2/50: loss=1.5473, test_acc=0.5855, time=9.8s\n",
            "Final Train Epoch 3/50: loss=0.9162, test_acc=0.6771, time=9.7s\n",
            "Final Train Epoch 4/50: loss=0.7186, test_acc=0.7520, time=9.7s\n",
            "Final Train Epoch 5/50: loss=0.6150, test_acc=0.7768, time=9.8s\n",
            "Final Train Epoch 6/50: loss=0.5669, test_acc=0.7956, time=9.6s\n",
            "Final Train Epoch 7/50: loss=0.5382, test_acc=0.8021, time=9.8s\n",
            "Final Train Epoch 8/50: loss=0.5173, test_acc=0.8091, time=9.6s\n",
            "Final Train Epoch 9/50: loss=0.4995, test_acc=0.8192, time=9.7s\n",
            "Final Train Epoch 10/50: loss=0.4837, test_acc=0.8200, time=9.6s\n",
            "Final Train Epoch 11/50: loss=0.4697, test_acc=0.8273, time=9.8s\n",
            "Final Train Epoch 12/50: loss=0.4566, test_acc=0.8311, time=9.9s\n",
            "Final Train Epoch 13/50: loss=0.4435, test_acc=0.8281, time=9.7s\n",
            "Final Train Epoch 14/50: loss=0.4318, test_acc=0.8333, time=9.7s\n",
            "Final Train Epoch 15/50: loss=0.4222, test_acc=0.8406, time=9.8s\n",
            "Final Train Epoch 16/50: loss=0.4123, test_acc=0.8373, time=9.6s\n",
            "Final Train Epoch 17/50: loss=0.4050, test_acc=0.8469, time=9.8s\n",
            "Final Train Epoch 18/50: loss=0.3952, test_acc=0.8446, time=9.7s\n",
            "Final Train Epoch 19/50: loss=0.3891, test_acc=0.8425, time=9.8s\n",
            "Final Train Epoch 20/50: loss=0.3821, test_acc=0.8512, time=9.7s\n",
            "Final Train Epoch 21/50: loss=0.3759, test_acc=0.8513, time=9.8s\n",
            "Final Train Epoch 22/50: loss=0.3705, test_acc=0.8521, time=9.6s\n",
            "Final Train Epoch 23/50: loss=0.3654, test_acc=0.8520, time=9.6s\n",
            "Final Train Epoch 24/50: loss=0.3598, test_acc=0.8493, time=9.7s\n",
            "Final Train Epoch 25/50: loss=0.3552, test_acc=0.8594, time=9.7s\n",
            "Final Train Epoch 26/50: loss=0.3510, test_acc=0.8528, time=9.6s\n",
            "Final Train Epoch 27/50: loss=0.3469, test_acc=0.8541, time=9.7s\n",
            "Final Train Epoch 28/50: loss=0.3427, test_acc=0.8575, time=9.9s\n",
            "Final Train Epoch 29/50: loss=0.3390, test_acc=0.8586, time=9.7s\n",
            "Final Train Epoch 30/50: loss=0.3362, test_acc=0.8635, time=9.6s\n",
            "Final Train Epoch 31/50: loss=0.3323, test_acc=0.8586, time=9.6s\n",
            "Final Train Epoch 32/50: loss=0.3292, test_acc=0.8594, time=9.7s\n",
            "Final Train Epoch 33/50: loss=0.3259, test_acc=0.8547, time=9.7s\n",
            "Final Train Epoch 34/50: loss=0.3232, test_acc=0.8655, time=9.7s\n",
            "Final Train Epoch 35/50: loss=0.3202, test_acc=0.8577, time=9.6s\n",
            "Final Train Epoch 36/50: loss=0.3181, test_acc=0.8622, time=9.7s\n",
            "Final Train Epoch 37/50: loss=0.3153, test_acc=0.8633, time=9.8s\n",
            "Final Train Epoch 38/50: loss=0.3125, test_acc=0.8653, time=9.6s\n",
            "Final Train Epoch 39/50: loss=0.3092, test_acc=0.8633, time=9.7s\n",
            "Final Train Epoch 40/50: loss=0.3067, test_acc=0.8567, time=9.8s\n",
            "Final Train Epoch 41/50: loss=0.3040, test_acc=0.8660, time=9.8s\n",
            "Final Train Epoch 42/50: loss=0.3023, test_acc=0.8656, time=9.6s\n",
            "Final Train Epoch 43/50: loss=0.3001, test_acc=0.8645, time=9.8s\n",
            "Final Train Epoch 44/50: loss=0.2980, test_acc=0.8640, time=9.8s\n",
            "Final Train Epoch 45/50: loss=0.2948, test_acc=0.8650, time=9.7s\n",
            "Final Train Epoch 46/50: loss=0.2928, test_acc=0.8653, time=9.6s\n",
            "Final Train Epoch 47/50: loss=0.2911, test_acc=0.8659, time=9.6s\n",
            "Final Train Epoch 48/50: loss=0.2887, test_acc=0.8627, time=9.7s\n",
            "Final Train Epoch 49/50: loss=0.2868, test_acc=0.8662, time=9.6s\n",
            "Final Train Epoch 50/50: loss=0.2846, test_acc=0.8647, time=9.7s\n",
            "Length of parameter history: 50\n",
            "Final Test Accuracy: 0.8647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_enhanced_mlp(input_dim, n_layers, hidden_units, output_dim,\n",
        "                       use_batchnorm=False, dropout_rate=0.0, activation='relu'):\n",
        "    \"\"\"Build enhanced MLP with optional BatchNorm, Dropout, and different activations.\"\"\"\n",
        "    layers = []\n",
        "\n",
        "    activations = {\n",
        "        'relu': nn.ReLU(),\n",
        "        'leaky': nn.LeakyReLU(0.01),\n",
        "        'elu': nn.ELU(),\n",
        "        'prelu': nn.PReLU()\n",
        "    }\n",
        "    act_fn = activations[activation]\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        layers.append(nn.Linear(input_dim, hidden_units))\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm1d(hidden_units))\n",
        "\n",
        "        layers.append(act_fn if activation != 'prelu' else nn.PReLU())\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        input_dim = hidden_units\n",
        "\n",
        "    layers.append(nn.Linear(input_dim, output_dim))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def enhanced_hyperparam_tuning(train_dataset, val_dataset, seed=541):\n",
        "    \"\"\"Find best enhanced architecture config.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    best_cfg = None\n",
        "    best_acc = 0.0\n",
        "\n",
        "    batch_sizes = [32, 64, 128]\n",
        "    learning_rates = [0.001, 0.0005, 0.0001]\n",
        "    layers = [3, 4, 5]\n",
        "    hidden = [64, 128, 256]\n",
        "    alphas = [0, 0.0001, 0.00001]\n",
        "    epochs = [30, 40]\n",
        "\n",
        "    use_batchnorm_options = [True, False]\n",
        "    dropout_rates = [0.0, 0.2, 0.3, 0.5]\n",
        "    activations = ['relu', 'leaky', 'elu', 'prelu']\n",
        "\n",
        "    x0, y0 = train_dataset[0]\n",
        "    n_features = x0.numel()\n",
        "    n_classes = 10\n",
        "\n",
        "    loopcount = 0\n",
        "    config_attempts = 15\n",
        "\n",
        "    for _ in range(config_attempts):\n",
        "\n",
        "        layer_choice = int(np.random.choice(layers))\n",
        "        batch_size_choice = int(np.random.choice(batch_sizes))\n",
        "        learning_rate_choice = float(np.random.choice(learning_rates))\n",
        "        hidden_choice = int(np.random.choice(hidden))\n",
        "        alpha_choice = float(np.random.choice(alphas))\n",
        "        epoch_choice = int(np.random.choice(epochs))\n",
        "        use_bn = bool(np.random.choice(use_batchnorm_options))\n",
        "        dropout = float(np.random.choice(dropout_rates))\n",
        "        activation = np.random.choice(activations)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size_choice, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size_choice, shuffle=False)\n",
        "\n",
        "        torch.manual_seed(seed + loopcount) # SEED FIX\n",
        "        model = build_enhanced_mlp(n_features, layer_choice, hidden_choice, n_classes,\n",
        "                                  use_batchnorm=use_bn, dropout_rate=dropout, activation=activation)\n",
        "\n",
        "        weight_params = []\n",
        "        bias_params = []\n",
        "        for name, p in model.named_parameters():\n",
        "            (weight_params if 'weight' in name else bias_params).append(p)\n",
        "\n",
        "        optimizer = torch.optim.SGD([\n",
        "            {'params': weight_params, 'weight_decay': alpha_choice},\n",
        "            {'params': bias_params, 'weight_decay': 0.0}\n",
        "        ], lr=learning_rate_choice)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for _e in range(epoch_choice):\n",
        "            train_epoch(model, train_loader, criterion, optimizer)\n",
        "\n",
        "        current_accuracy = evaluate(model, val_loader)\n",
        "\n",
        "        loopcount = loopcount + 1\n",
        "        print(\"on enhanced loop \", loopcount)\n",
        "\n",
        "        if current_accuracy > best_acc:\n",
        "            best_acc = current_accuracy\n",
        "            print(\"best enhanced accuracy \", best_acc)\n",
        "\n",
        "            best_cfg = {\n",
        "                \"layers\": layer_choice,\n",
        "                \"hidden\": hidden_choice,\n",
        "                \"batch\": batch_size_choice,\n",
        "                \"learning_rate\": learning_rate_choice,\n",
        "                \"alpha\": alpha_choice,\n",
        "                \"epochs\": epoch_choice,\n",
        "                \"batchnorm\": use_bn,\n",
        "                \"dropout\": dropout,\n",
        "                \"activation\": activation,\n",
        "                \"seed\": seed + loopcount - 1  # SEED FIX\n",
        "            }\n",
        "\n",
        "            print(\"New best enhanced:\", best_cfg, \"val_acc=\", f\"{best_acc:.4f}\")\n",
        "\n",
        "    return best_cfg, best_acc\n",
        "\n",
        "\n",
        "enhanced_cfg, enhanced_acc = enhanced_hyperparam_tuning(train_dataset, val_dataset)\n",
        "print(f\"Enhanced config: {enhanced_cfg} with val_acc={enhanced_acc:.4f}\")\n",
        "\n",
        "# Train final enhanced model\n",
        "torch.manual_seed(enhanced_cfg['seed']) # SEED FIX\n",
        "enhanced_model = build_enhanced_mlp(\n",
        "    n_features,\n",
        "    enhanced_cfg['layers'],\n",
        "    enhanced_cfg['hidden'],\n",
        "    n_classes,\n",
        "    use_batchnorm=enhanced_cfg['batchnorm'],\n",
        "    dropout_rate=enhanced_cfg['dropout'],\n",
        "    activation=enhanced_cfg['activation']\n",
        ")\n",
        "\n",
        "batch_size = enhanced_cfg['batch']\n",
        "full_train_loader = DataLoader(full_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "weight_params = []\n",
        "bias_params = []\n",
        "for name, p in enhanced_model.named_parameters():\n",
        "    (weight_params if 'weight' in name else bias_params).append(p)\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': weight_params, 'weight_decay': enhanced_cfg['alpha']},\n",
        "    {'params': bias_params, 'weight_decay': 0.0}\n",
        "], lr=enhanced_cfg['learning_rate'])\n",
        "\n",
        "# retrain enhanced model\n",
        "epochs_final = 50\n",
        "for epoch in range(epochs_final):\n",
        "    t0 = __import__('time').time(); loss, _ = train_epoch(enhanced_model, full_train_loader, criterion, optimizer)\n",
        "    test_acc = evaluate(enhanced_model, test_loader); t1 = __import__('time').time()\n",
        "    print(f\"Enhanced Train Epoch {epoch+1}/{epochs_final}: loss={loss:.4f}, test_acc={test_acc:.4f}, time={t1-t0:.1f}s\")\n",
        "\n",
        "print(f\"Final Enhanced Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "be6_8ww7UYdh",
        "outputId": "cc62845c-8785-440c-9dfc-3e9489e6f4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "be6_8ww7UYdh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on enhanced loop  1\n",
            "best enhanced accuracy  0.7305833333333334\n",
            "New best enhanced: {'layers': 5, 'hidden': 64, 'batch': 32, 'learning_rate': 0.001, 'alpha': 0.0001, 'epochs': 30, 'batchnorm': False, 'dropout': 0.0, 'activation': np.str_('relu'), 'seed': 541} val_acc= 0.7306\n",
            "on enhanced loop  2\n",
            "best enhanced accuracy  0.8628333333333333\n",
            "New best enhanced: {'layers': 4, 'hidden': 64, 'batch': 64, 'learning_rate': 0.001, 'alpha': 0.0, 'epochs': 30, 'batchnorm': True, 'dropout': 0.2, 'activation': np.str_('leaky'), 'seed': 542} val_acc= 0.8628\n",
            "on enhanced loop  3\n",
            "on enhanced loop  4\n",
            "on enhanced loop  5\n",
            "on enhanced loop  6\n",
            "best enhanced accuracy  0.8870833333333333\n",
            "New best enhanced: {'layers': 4, 'hidden': 256, 'batch': 32, 'learning_rate': 0.001, 'alpha': 0.0001, 'epochs': 30, 'batchnorm': True, 'dropout': 0.2, 'activation': np.str_('relu'), 'seed': 546} val_acc= 0.8871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "F0sEF1223TrJJhervBRmvc8C",
      "metadata": {
        "tags": [],
        "id": "F0sEF1223TrJJhervBRmvc8C"
      },
      "source": [
        "# Prob 3\n",
        "def plotPath(loader, trajectory, model):\n",
        "    # TODO: change this toy plot to show a 2-d projection of the weight space\n",
        "    # along with the associated loss (cross-entropy), plus a superimposed\n",
        "    # trajectory across the landscape that was traversed using SGD. Use\n",
        "    # sklearn.decomposition.PCA's fit_transform and inverse_transform methods.\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Setup PCA and subset loader\n",
        "    trajectory = np.array(trajectory)\n",
        "    pca = PCA(n_components=2)\n",
        "    trajectory_2d = pca.fit_transform(trajectory)\n",
        "\n",
        "    subset_size = min(2500, len(loader.dataset))\n",
        "    subset_indices = np.random.choice(len(loader.dataset), subset_size, replace=False)\n",
        "    subset_dataset = torch.utils.data.Subset(loader.dataset, subset_indices)\n",
        "    subset_loader = DataLoader(subset_dataset, batch_size=256, shuffle=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def computeCELoss(x1, x2):\n",
        "        point_2d = np.array([[x1, x2]])\n",
        "        point_params = pca.inverse_transform(point_2d)[0]\n",
        "        load_params_into_model(model, torch.from_numpy(point_params).float())\n",
        "        return compute_loss(model, subset_loader, criterion)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "    # Compute the CE loss on a grid of points (corresonding to different w).\n",
        "    x_range = trajectory_2d[:, 0].max() - trajectory_2d[:, 0].min()\n",
        "    y_range = trajectory_2d[:, 1].max() - trajectory_2d[:, 1].min()\n",
        "    axis1 = np.arange(trajectory_2d[:, 0].min() - x_range*0.2, trajectory_2d[:, 0].max() + x_range*0.2, x_range/20)  # PCA space grid\n",
        "    axis2 = np.arange(trajectory_2d[:, 1].min() - y_range*0.2, trajectory_2d[:, 1].max() + y_range*0.2, y_range/20)  # PCA space grid\n",
        "    Xaxis, Yaxis = np.meshgrid(axis1, axis2)\n",
        "    Zaxis = np.zeros((len(axis1), len(axis2)))\n",
        "    for i in tqdm.tqdm(range(len(axis1))):\n",
        "        for j in range(len(axis2)):\n",
        "            Zaxis[i,j] = computeCELoss(Xaxis[i,j], Yaxis[i,j])\n",
        "    ax.plot_surface(Xaxis, Yaxis, Zaxis, alpha=0.6)  # Keep alpha < 1 so we can see the scatter plot too.\n",
        "\n",
        "    # Now superimpose a scatter plot showing the weights during SGD.\n",
        "    Xaxis = trajectory_2d[:, 0]  # SGD trajectory in PCA space\n",
        "    Yaxis = trajectory_2d[:, 1]  # SGD trajectory in PCA space\n",
        "    Zaxis = np.array([computeCELoss(Xaxis[i], Yaxis[i]) for i in range(len(Xaxis))])\n",
        "    ax.scatter(Xaxis, Yaxis, Zaxis, color='r')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Problem 3: Visualizing the loss landscape and optimization trajectories\n",
        "# Plot the trajectory of parameters during training\n",
        "# Uncomment to run\n",
        "# plotPath(full_train_loader, parameter_history, best_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "cs541_hw3_p2-3_d1a",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}